# # Student Dropout Prediction – Prototype
# Dataset: UCI "Predict Students' Dropout and Academic Success" (ID: 697)


# 1. Imports

import numpy as np
import pandas as pd

from ucimlrepo import fetch_ucirepo  # pip install ucimlrepo

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay
)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

import matplotlib.pyplot as plt

# %%
# 2. Load dataset from UCI

dataset = fetch_ucirepo(id=697)  # Predict Students' Dropout and Academic Success

X = dataset.data.features
y = dataset.data.targets

print("Features shape:", X.shape)
print("Targets shape:", y.shape)
print("Target columns:", y.columns)

# For this dataset, the target column is often named something like 'Target'
# Show first few rows to confirm:
display(y.head())

# %%
# 3. Prepare features (X) and target (y)

# If there is only one target column, convert to 1D array
if y.shape[1] == 1:
    y = y.iloc[:, 0]
else:
    # If there are multiple target columns, pick the main status column
    # Adjust this line to the actual column name if needed
    y = y['Target']  # change if your column is named differently

print("Unique target values:", y.unique())

# %%
# 4. Train-test split (80/20 recommended by dataset authors)

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

# %%
# 5. Identify numeric and categorical columns

numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()

print("Numeric columns:", len(numeric_cols))
print("Categorical columns:", len(categorical_cols))

# %%
# 6. Preprocessing pipeline

numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ]
)

# %%
# 7. Baseline model – Multinomial Logistic Regression

log_reg = LogisticRegression(
    multi_class="multinomial",
    max_iter=1000,
    class_weight="balanced"  # handle class imbalance
)

log_reg_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", log_reg)
])

log_reg_clf.fit(X_train, y_train)

y_pred_log = log_reg_clf.predict(X_test)

print("=== Logistic Regression – Test Performance ===")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

cm_log = confusion_matrix(y_test, y_pred_log, labels=y.unique())
disp_log = ConfusionMatrixDisplay(confusion_matrix=cm_log, display_labels=y.unique())
disp_log.plot(xticks_rotation=45)
plt.title("Logistic Regression – Confusion Matrix")
plt.tight_layout()
plt.show()

# %%
# 8. Tree-based model – Decision Tree

dt = DecisionTreeClassifier(
    random_state=42,
    class_weight="balanced"
)

dt_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", dt)
])

dt_clf.fit(X_train, y_train)
y_pred_dt = dt_clf.predict(X_test)

print("=== Decision Tree – Test Performance ===")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

cm_dt = confusion_matrix(y_test, y_pred_dt, labels=y.unique())
disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=y.unique())
disp_dt.plot(xticks_rotation=45)
plt.title("Decision Tree – Confusion Matrix")
plt.tight_layout()
plt.show()

# %%
# 9. Advanced model – Random Forest

rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    random_state=42,
    class_weight="balanced_subsample",
    n_jobs=-1
)

rf_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", rf)
])

rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)

print("=== Random Forest – Test Performance ===")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

cm_rf = confusion_matrix(y_test, y_pred_rf, labels=y.unique())
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=y.unique())
disp_rf.plot(xticks_rotation=45)
plt.title("Random Forest – Confusion Matrix")
plt.tight_layout()
plt.show()

# %%
# 10. Cross-validation on training set (optional, for robustness)

cv_scores = cross_val_score(
    rf_clf,
    X_train,
    y_train,
    cv=5,
    scoring="f1_weighted",
    n_jobs=-1
)

print("Random Forest 5-fold CV F1 (weighted):", cv_scores)
print("Mean F1:", cv_scores.mean(), "Std:", cv_scores.std())
